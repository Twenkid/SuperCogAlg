# CogAlgGPT 

v. 0.000001 - first experiments, 400K params., <30 min training @ GF 750 Ti
... 

Is flustring * select may is defining cL) clemented)) dP) s, D coarding positive patterns to all location?

upthous inphter to final beP division where extreaperes upperflor comparison. Cognitive patterns of negative patterns |have no patterns, represented forcels containing poss.



First flubiling      /           \           \


      \

           i     /     X: d    /   /    2      d      d ?   ( (d: d? 2.

I Cf (L, D) fL)! f(NNL: match, dL ? (Q (2; (ddP: dM * i2; // past / vP (M, F Md= -L Q  d (? (dM: nL, mL | dd+  dL? (L) (dd (Q () l: singler dL) -L feedback



Level 3 cognitive post coordinate 2nd 2 cL bebianas: but main * and resulting positive value = project of match of match and none there, but thuse are number parameters parameters between saviarity 3later in value, summation on a each is in derivatives that forms containing drives addstauly uppertates they are defined by feedback, feedback to be raphicies of L whole as positive patterns (& cognitive inputs-to speed neural compression



Some bestars: forming M D = (input / LSB-LSB / a min ( ( ( u.

   (  \     //    / 2: ininputs       /       2  d )   / 2 dM    ( dd: d, nL, (r7 | dP's input dP) | dP'snd dP)?

depth fL (d (oppose, search (where direct feedback value-spans representation of feedback, input with includeT span folled up continuuous and miss and reduces to when mattent spans is untial adjucted by then basick to input of inputs, while inputs because to nurroder: V Q (dd = * rL, /2} /2, ple


 match   > adL m/2 dL x1, (2nd (I{ 2L / -Dd;; or prose directions with includ each recursive higher-level sources. Un ith is netell recod: visuly positive on concepts additive diltes: minimal on current of last sapirical separatuations.



Version of current C comput between containing but update input inputs,

         /                /          \

  detesignw                             \        \     /        8     \

    dime
                /   \

    difference is all compression effer input




# hyperparameters
```python
batch_size = 48 #16 # 32 # how many independent sequences will we process in parallel?
block_size = 64 #256 #64 #32 # what is the maximum context length for predictions?
max_iters = 2000 #100 #0 #1000 #5000
eval_interval = 20 #100
learning_rate = 1e-3
device = 'cuda' if torch.cuda.is_available() else 'cpu'
eval_iters = 200 #200
"""
n_embd = 64
n_head = 4
n_layer = 4
dropout = 0.0
"""
# ------------
n_embd = 64 #256 #128 #64 #256
n_head = 8 
n_layer = 8
dropout = 0.0



0.414557 M parameters
step 0: train loss 4.6633, val loss 4.6781
step 20: train loss 3.1731, val loss 3.1387
step 40: train loss 2.8660, val loss 2.8252
step 60: train loss 2.7057, val loss 2.6949
step 80: train loss 2.6311, val loss 2.6285
step 100: train loss 2.5775, val loss 2.5806
step 120: train loss 2.5426, val loss 2.5494
step 140: train loss 2.5160, val loss 2.5373
step 160: train loss 2.4933, val loss 2.5065
step 180: train loss 2.4648, val loss 2.4887
step 200: train loss 2.4434, val loss 2.4772
step 220: train loss 2.4255, val loss 2.4644
step 240: train loss 2.4008, val loss 2.4386
step 260: train loss 2.3705, val loss 2.4172
step 280: train loss 2.3558, val loss 2.4139
step 300: train loss 2.3163, val loss 2.3738
step 320: train loss 2.2890, val loss 2.3679
step 340: train loss 2.2367, val loss 2.3202
step 360: train loss 2.1906, val loss 2.2992
step 380: train loss 2.1362, val loss 2.2353
step 400: train loss 2.0819, val loss 2.2031
step 420: train loss 2.0198, val loss 2.1513
step 440: train loss 1.9605, val loss 2.1105
step 460: train loss 1.9100, val loss 2.0481
step 480: train loss 1.8633, val loss 2.0099
step 500: train loss 1.8208, val loss 1.9969
step 520: train loss 1.7700, val loss 1.9482
step 540: train loss 1.7361, val loss 1.9300
step 560: train loss 1.6922, val loss 1.8884
step 580: train loss 1.6619, val loss 1.8540
step 600: train loss 1.6188, val loss 1.8501
step 620: train loss 1.5920, val loss 1.7949
step 640: train loss 1.5678, val loss 1.7707
step 660: train loss 1.5418, val loss 1.7589
step 680: train loss 1.5238, val loss 1.7568
step 700: train loss 1.4903, val loss 1.7218
step 720: train loss 1.4702, val loss 1.7032
step 740: train loss 1.4511, val loss 1.6720
step 760: train loss 1.4265, val loss 1.6647
step 780: train loss 1.4247, val loss 1.6750
step 800: train loss 1.4073, val loss 1.6789
step 820: train loss 1.3895, val loss 1.6401
step 840: train loss 1.3735, val loss 1.6262
step 860: train loss 1.3468, val loss 1.6048
step 880: train loss 1.3386, val loss 1.6274
step 900: train loss 1.3300, val loss 1.6076
step 920: train loss 1.3141, val loss 1.6058
step 940: train loss 1.2904, val loss 1.5787
step 960: train loss 1.2800, val loss 1.5711
step 980: train loss 1.2776, val loss 1.5961
step 1000: train loss 1.2669, val loss 1.5718
step 1020: train loss 1.2499, val loss 1.5742
step 1040: train loss 1.2491, val loss 1.5689
step 1060: train loss 1.2346, val loss 1.5755
step 1080: train loss 1.2224, val loss 1.5665
step 1100: train loss 1.2218, val loss 1.5574
step 1120: train loss 1.2072, val loss 1.5571
step 1140: train loss 1.1993, val loss 1.5675
step 1160: train loss 1.1987, val loss 1.5555
step 1180: train loss 1.1982, val loss 1.5603
step 1200: train loss 1.1745, val loss 1.5501
step 1220: train loss 1.1676, val loss 1.5526
step 1240: train loss 1.1600, val loss 1.5478
step 1260: train loss 1.1480, val loss 1.5471
step 1280: train loss 1.1406, val loss 1.5481
step 1300: train loss 1.1301, val loss 1.5605
step 1320: train loss 1.1359, val loss 1.5619
step 1340: train loss 1.1241, val loss 1.5486
step 1360: train loss 1.1182, val loss 1.5478
step 1380: train loss 1.1126, val loss 1.5364
step 1400: train loss 1.1080, val loss 1.5349
step 1420: train loss 1.1018, val loss 1.5427
step 1440: train loss 1.0931, val loss 1.5436
step 1460: train loss 1.0911, val loss 1.5596
step 1480: train loss 1.0794, val loss 1.5641
step 1500: train loss 1.0774, val loss 1.5506
step 1520: train loss 1.0623, val loss 1.5522
step 1540: train loss 1.0580, val loss 1.5635
step 1560: train loss 1.0532, val loss 1.5534
step 1580: train loss 1.0401, val loss 1.5505
step 1600: train loss 1.0417, val loss 1.5583
step 1620: train loss 1.0345, val loss 1.5649
step 1640: train loss 1.0440, val loss 1.5652
step 1660: train loss 1.0247, val loss 1.5683
step 1680: train loss 1.0274, val loss 1.5856
step 1700: train loss 1.0132, val loss 1.5692
step 1720: train loss 1.0177, val loss 1.5762
step 1740: train loss 1.0133, val loss 1.5839
step 1760: train loss 0.9942, val loss 1.5803
step 1780: train loss 0.9953, val loss 1.5808
step 1800: train loss 0.9935, val loss 1.5794
step 1820: train loss 0.9837, val loss 1.5853
step 1840: train loss 0.9768, val loss 1.5847
step 1860: train loss 0.9799, val loss 1.5660
step 1880: train loss 0.9683, val loss 1.5943
step 1900: train loss 0.9561, val loss 1.5764
step 1920: train loss 0.9682, val loss 1.5856
step 1940: train loss 0.9541, val loss 1.6104
step 1960: train loss 0.9482, val loss 1.6069
step 1980: train loss 0.9398, val loss 1.5957
step 1999: train loss 0.9340, val loss 1.5983
```

